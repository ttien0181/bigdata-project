# ğŸŒ¤ï¸ Weather Monitoring System - Complete Setup Guide

> **HÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c, cháº¡y trÃªn Minikube (Windows)**  
> PhÃ¹ há»£p cho hÃ´m sau khi báº­t PC lÃªn, cÃ³ thá»ƒ cháº¡y tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i khÃ´ng bá»‹ lá»—i

---

## ğŸ“‹ Kiá»ƒm Tra TrÆ°á»›c Khi Báº¯t Äáº§u

Má»Ÿ **PowerShell** vÃ  cháº¡y cÃ¡c lá»‡nh sau Ä‘á»ƒ kiá»ƒm tra Ä‘iá»u kiá»‡n:

```powershell
# 1. Kiá»ƒm tra Docker
docker version

# 2. Kiá»ƒm tra Minikube
minikube status

# 3. Kiá»ƒm tra kubectl
kubectl version --client

# 4. Kiá»ƒm tra thÆ° má»¥c project
cd d:\BigData\bigdata
ls
```

**Náº¿u Minikube chÆ°a cháº¡y:**
```powershell
minikube delete  # XÃ³a cluster cÅ© (náº¿u cáº§n reset)
minikube start --cpus=4 --memory=8192 --disk-size=40g
minikube status  # Kiá»ƒm tra

# âš ï¸ IMPORTANT: Táº¡o data directories trong Minikube VM
minikube ssh "sudo mkdir -p /data/postgres /data/kafka /data/namenode /data/datanode /data/zookeeper && sudo chmod 777 /data/*"
```

âœ… **Äiá»u kiá»‡n sáºµn sÃ ng:**
- Docker Desktop cháº¡y bÃ¬nh thÆ°á»ng  
- Minikube Status = `Running`
- kubectl cÃ³ thá»ƒ gá»i Ä‘Æ°á»£c

---

## â±ï¸ Thá»i Gian Dá»± Kiáº¿n (Tá»•ng ~60 phÃºt)

| BÆ°á»›c | MÃ´ táº£ | Thá»i gian |
|------|-------|----------|
| 1-2 | Setup Kubernetes + Kafka | 5 phÃºt |
| 3-5 | Deploy HDFS, Spark, PostgreSQL | 8 phÃºt |
| 6 | Build Docker images | 5 phÃºt |
| 7-8 | Deploy + Verify apps | 5 phÃºt |
| 9-10 | Lambda Architecture + Batch | 30 phÃºt |
| **Tá»”NG** | | **~53 phÃºt** |

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng (Lambda Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Producer   â”‚â”€â”€â”€â”€â”€â–¶â”‚   Kafka   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Spark Streaming    â”‚
â”‚  (Python)   â”‚      â”‚ (Strimzi) â”‚      â”‚  (stream_app.py)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                                                â”‚          â”‚
                                        SPEED LAYER   BATCH LAYER
                                                â”‚          â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚                                     â”‚
                        PostgreSQL â—€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HDFS /batch/     â”‚
                       weather_final              â”‚      daily_stats      â”‚
                                           SERVING LAYERâ—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                    PostgreSQL â—€â”˜
                                   weather_daily_stats
```

---

## ğŸš€ BÆ¯á»šC 1-2: Setup Kubernetes & Kafka

**Má»¥c Ä‘Ã­ch:** Khá»Ÿi táº¡o cluster vÃ  Kafka broker

âš ï¸ **LÆ°u Ã½ Minikube:** Kafka sá»­ dá»¥ng ephemeral storage (emptyDir) Ä‘á»ƒ trÃ¡nh lá»—i permission trÃªn Minikube hostPath PV. Dá»¯ liá»‡u Kafka sáº½ máº¥t khi pod restart. PhÃ¹ há»£p cho testing/demo.

```powershell
# 1. Táº¡o namespace
kubectl create namespace air-quality

# 2. CÃ i Strimzi Operator (Kafka controller)
kubectl create -f 'https://strimzi.io/install/latest?namespace=air-quality' -n air-quality

# 3. Chá» Strimzi ready (~30 giÃ¢y)
kubectl wait deployment strimzi-cluster-operator --for=condition=available --timeout=300s -n air-quality

# 4. Deploy services & Kafka
kubectl apply -f k8s/00-namespace-config.yaml
kubectl apply -f k8s/01-services.yaml
kubectl apply -f k8s/kafka-strimzi.yaml

# 5. Chá» Kafka ready (~2-3 phÃºt) â³
Write-Host "Waiting for Kafka..." -ForegroundColor Yellow
kubectl wait kafka/air-quality-kafka --for=condition=Ready --timeout=300s -n air-quality

# âœ“ Verify Kafka
kubectl get kafka -n air-quality
```

**Expected output:** Kafka vá»›i `Ready` = `True`

---

## ğŸ¢ BÆ¯á»šC 3: Deploy HDFS (Storage)

**Má»¥c Ä‘Ã­ch:** LÆ°u stream data (/stream/) vÃ  batch aggregations (/batch/daily_stats/)

```powershell
# Deploy HDFS
kubectl apply -f k8s/03-hadoop.yaml

# Chá» HDFS ready (~2 phÃºt) â³
Write-Host "Waiting for HDFS..." -ForegroundColor Yellow
kubectl wait --for=condition=ready pod -l app=namenode -n air-quality --timeout=300s

# âœ“ Verify HDFS pods
kubectl get pods -n air-quality 
```

**Expected output:** `namenode-0` vÃ  `datanode-0` running

---

## âš¡ BÆ¯á»šC 4: Deploy Spark Cluster

**Má»¥c Ä‘Ã­ch:** Processing engine cho streaming + batch

```powershell
# Deploy Spark Master & Workers
kubectl apply -f k8s/04-spark.yaml

# Chá» Spark ready (~1 phÃºt) â³
Write-Host "Waiting for Spark..." -ForegroundColor Yellow
kubectl wait --for=condition=ready pod -l app=spark-master -n air-quality --timeout=300s

# âœ“ Verify Spark
kubectl get pods -n air-quality | Select-String spark
```

**Expected output:** `spark-master-0` + `spark-worker-0-x` + `spark-worker-1-x` running

---

## ğŸ—„ï¸ BÆ¯á»šC 5: Deploy PostgreSQL & Create Tables

**Má»¥c Ä‘Ã­ch:** OLTP database cho real-time + analytical queries

```powershell
# Deploy PostgreSQL
kubectl apply -f k8s/05-database.yaml

# Chá» PostgreSQL ready (~1 phÃºt) â³
Write-Host "Waiting for PostgreSQL..." -ForegroundColor Yellow
kubectl wait --for=condition=ready pod -l app=postgres -n air-quality --timeout=300s

# Táº¡o database weather_data
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d postgres -c "CREATE DATABASE weather_data;"

# Táº¡o table weather_final (Speed Layer - real-time)
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "CREATE TABLE IF NOT EXISTS weather_final (timestamp timestamp, ingested_at timestamp, longitude double precision, latitude double precision, temperature double precision, feels_like double precision, humidity int, pressure int, city varchar(50));"

# Táº¡o table weather_daily_stats (Batch Layer - aggregations)
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "CREATE TABLE IF NOT EXISTS weather_daily_stats (date date NOT NULL, city varchar(50) NOT NULL, avg_temperature double precision, min_temperature double precision, max_temperature double precision, avg_humidity double precision, min_humidity double precision, max_humidity double precision, avg_pressure double precision, record_count bigint, PRIMARY KEY (date, city)); CREATE INDEX IF NOT EXISTS idx_daily_stats_date ON weather_daily_stats(date DESC); CREATE INDEX IF NOT EXISTS idx_daily_stats_city ON weather_daily_stats(city);"

# âœ“ Verify tables
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "\dt"
```

**Expected output:** 2 tables: `weather_final` + `weather_daily_stats`

---

## ğŸ³ BÆ¯á»šC 6: Build Docker Images

**Má»¥c Ä‘Ã­ch:** Táº¡o images cho Producer (v3) vÃ  Spark Processor (v7 vá»›i batch scripts)

```powershell
# IMPORTANT: Point Docker to Minikube (Ä‘á»ƒ build image trong Minikube)
Write-Host "Pointing Docker to Minikube..." -ForegroundColor Cyan
minikube -p minikube docker-env --shell powershell | Invoke-Expression

# Build Producer (v3)
Write-Host "Building producer:v3..." -ForegroundColor Cyan
docker build -t producer:v3 ./producer

# Build Spark Processor (v7 - includes batch_job.py + serving_layer.py)
Write-Host "Building spark-processor:v7..." -ForegroundColor Cyan
docker build -t spark-processor:v7 ./spark-processor

# âœ“ Verify images
Write-Host "Verifying images..." -ForegroundColor Green
docker images | Select-String "producer\|spark-processor"
```

**Expected output:** Tháº¥y cáº£ `producer:v3` vÃ  `spark-processor:v7`

---

## ğŸš€ BÆ¯á»šC 7-8: Deploy Applications & Kafka Topic

**Má»¥c Ä‘Ã­ch:** Cháº¡y Producer â†’ Kafka, Spark Processor xá»­ lÃ½ real-time

```powershell
# Deploy Kafka topic trÆ°á»›c
kubectl apply -f k8s/07-kafka-topic-weather.yaml

# Deploy Producer + Spark Processor
kubectl apply -f k8s/06-applications.yaml

# Chá» pods ready (~1 phÃºt) â³
Write-Host "Waiting for Producer & Spark Processor..." -ForegroundColor Yellow
kubectl wait --for=condition=ready pod -l app=producer -n air-quality --timeout=300s
kubectl wait --for=condition=ready pod -l app=spark-processor -n air-quality --timeout=300s

# âœ“ Verify deployment
kubectl get pods -n air-quality | Select-String "producer\|spark-processor"
```

**Expected output:** Both pods `Running`

---

## âœ“ BÆ¯á»šC 9: Verify Real-Time Data Flow (Speed Layer)

**Má»¥c Ä‘Ã­ch:** Kiá»ƒm tra Producer â†’ Kafka â†’ Spark â†’ PostgreSQL/HDFS

```powershell
# 1. Check Producer logs
Write-Host "`nğŸ“¤ Producer logs:" -ForegroundColor Green
kubectl logs -f deployment/producer -n air-quality --tail=5
# (Chá» 5 giÃ¢y rá»“i Ctrl+C)
# Expected: "Sending data for Hanoi...", "Sending data for HCM...", etc.

# 2. Check Spark logs
Write-Host "`nâš¡ Spark logs:" -ForegroundColor Green
kubectl logs deployment/spark-processor -n air-quality --tail=15
# Expected: "=== Batch X ===" vá»›i data rows

# 3. Check HDFS data
kubectl exec -it namenode-0 -n air-quality -- sh -c "hdfs dfs -ls /data/weather_data/ | tail -10"
# Expected: Many .snappy.parquet files

# 4. Check PostgreSQL data (Speed Layer)
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "SELECT COUNT(*) FROM weather_final;"
# Expected: count > 30
```

---

## ğŸ—ï¸ BÆ¯á»šC 10: Lambda Architecture - Batch Processing

### 10.1 Chá» Stream Data (10-15 phÃºt)

```powershell
# Kiá»ƒm tra stream data folder Ä‘Æ°á»£c táº¡o
kubectl exec -it namenode-0 -n air-quality -- sh -c "hdfs dfs -ls /data/weather_data/stream/ | tail -5"

# Náº¿u chÆ°a cÃ³, chá» 10 phÃºt, stream app sáº½ táº¡o nÃ³
# Náº¿u váº«n khÃ´ng cÃ³, kiá»ƒm tra logs:
kubectl logs deployment/spark-processor -n air-quality | Select-String "write parquet to STREAM"
```

### 10.2 Run Batch Job (Aggregate Daily Stats)

```powershell
# cháº¡y lá»‡nh láº¥y pods
kubectl get pods -n air-quality

# Láº¥y tÃªn pod spark-processor (pod cÃ³ Spark + Python runtime)
$SPARK_POD = kubectl get pods -n air-quality -l app=spark-processor -o jsonpath="{.items[0].metadata.name}"


# Cháº¡y Spark batch job (Python sáº½ tá»± káº¿t ná»‘i Spark cluster)
kubectl exec -it $SPARK_POD -n air-quality -- python3 /opt/spark-apps/batch_job.py


# Verify batch output in HDFS
kubectl exec -it namenode-0 -n air-quality -- sh -c "hdfs dfs -ls /data/weather_data/batch/daily_stats/ | tail -10"
```

### 10.3 Run Serving Layer (Load Batch to PostgreSQL)

```powershell
Write-Host "`nğŸ“Š Running Serving Layer (load to PostgreSQL)..." -ForegroundColor Green

# Get spark-processor pod name
$SPARK_POD = kubectl get pods -n air-quality -l app=spark-processor -o jsonpath='{.items[0].metadata.name}'

# Run serving layer
kubectl exec -it $SPARK_POD -n air-quality -- python3 /opt/spark-apps/serving_layer.py
```

**Note:** The serving layer uses pandas + sqlalchemy instead of JDBC for simpler PostgreSQL integration.

```powershell
# Verify daily stats in PostgreSQL
Write-Host "`nâœ“ Daily statistics in PostgreSQL:" -ForegroundColor Green
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "SELECT date, city, ROUND(avg_temperature::numeric, 2) as avg_temp, ROUND(min_temperature::numeric, 2) as min_temp, ROUND(max_temperature::numeric, 2) as max_temp, record_count FROM weather_daily_stats ORDER BY date DESC, city;"
```

**Expected output:** Daily aggregations for each city by date

---

## ğŸ¨ BÆ¯á»šC 11 (Optional): Grafana Dashboard

```powershell
# Cháº¡y lá»‡nh:
kubectl port-forward svc/grafana -n air-quality 3000:3000


# Giá»¯ terminal KHÃ”NG ÄÃ“NG.

# Má»Ÿ trÃ¬nh duyá»‡t:
http://localhost:3000


ğŸ‘‰ Grafana sáº½ má»Ÿ ra ngay

# Login: admin / admin123
# 
# Add PostgreSQL Data Source:
# - Host: postgres:5432
# - Database: weather_data
# - User: admin
# - Password: password123
# - SSL Mode: disable
#
# Create Panel:
# SELECT 
#   timestamp as time,
#   temperature,
#   city
# FROM weather_final 
# WHERE timestamp > now() - interval '1 hour'
# ORDER BY timestamp DESC
```

---

## âœ… Verification Checklist

Cháº¡y script nÃ y Ä‘á»ƒ xÃ¡c minh táº¥t cáº£ Ä‘Ã£ hoÃ n thÃ nh:

```powershell
Write-Host "=== SYSTEM VERIFICATION ===" -ForegroundColor Cyan

Write-Host "`n1ï¸âƒ£ Pods Status:" -ForegroundColor Green
kubectl get pods -n air-quality | Select-String "producer|spark-processor|kafka|namenode|datanode|postgres|spark-master|spark-worker"

Write-Host "`n2ï¸âƒ£ Kafka Topic:" -ForegroundColor Green
kubectl get kafkatopic -n air-quality

Write-Host "`n3ï¸âƒ£ PostgreSQL Tables:" -ForegroundColor Green
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "\dt"

Write-Host "`n4ï¸âƒ£ Real-time Data (weather_final):" -ForegroundColor Green
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "SELECT COUNT(*) as count, COUNT(DISTINCT city) as cities FROM weather_final;"

Write-Host "`n5ï¸âƒ£ Daily Stats (weather_daily_stats):" -ForegroundColor Green
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data -c "SELECT COUNT(*) as days_aggregated FROM weather_daily_stats;"

Write-Host "`n6ï¸âƒ£ HDFS Stream Data:" -ForegroundColor Green
kubectl exec -it namenode-0 -n air-quality -- hdfs dfs -du -h /data/weather_data/stream/ | head -3

Write-Host "`nâœ… System is Ready!" -ForegroundColor Cyan
```

---

## ğŸ“š File Structure

```
bigdata/
â”œâ”€â”€ README.md                        â† Báº¡n Ä‘ang Ä‘á»c cÃ¡i nÃ y
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ sensor_sim.py               (Kafka producer)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ spark-processor/
â”‚   â”œâ”€â”€ stream_app.py               (Speed layer)
â”‚   â”œâ”€â”€ batch_job.py                (Batch layer - NEW)
â”‚   â”œâ”€â”€ serving_layer.py            (Serving layer - NEW)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ k8s/
    â”œâ”€â”€ 00-namespace-config.yaml
    â”œâ”€â”€ 01-services.yaml
    â”œâ”€â”€ 03-hadoop.yaml
    â”œâ”€â”€ 04-spark.yaml
    â”œâ”€â”€ 05-database.yaml
    â”œâ”€â”€ 06-applications.yaml        (Deploy apps with v7 image)
    â”œâ”€â”€ 07-kafka-topic-weather.yaml
    â””â”€â”€ kafka-strimzi.yaml
```

---

## ğŸ” Useful Commands

```powershell
# Check pod status
kubectl get pods -n air-quality

# View logs
kubectl logs -f deployment/producer -n air-quality
kubectl logs -f deployment/spark-processor -n air-quality

# Connect to containers
kubectl exec -it namenode-0 -n air-quality -- bash
kubectl exec -it postgres-0 -n air-quality -- psql -U admin -d weather_data

# HDFS operations
kubectl exec -it namenode-0 -n air-quality -- hdfs dfs -ls /data/weather_data/

# Restart pods
kubectl rollout restart deployment/producer -n air-quality
kubectl rollout restart deployment/spark-processor -n air-quality

# Check resources
kubectl top nodes -n air-quality
kubectl top pods -n air-quality

# Delete resources
kubectl delete namespace air-quality
```

---

## ğŸš¨ Quick Troubleshooting

| Problem | Solution |
|---------|----------|
| **Pods Pending** | `minikube delete && minikube start --memory=12288` |
| **Producer no data** | `kubectl rollout restart deployment/producer -n air-quality` |
| **No /stream/ folder** | Wait 10-15 min for spark to create it |
| **PostgreSQL connection error** | `kubectl logs statefulset/postgres -n air-quality` |
| **HDFS error** | `kubectl exec -it namenode-0 -n air-quality -- hdfs dfsadmin -report` |

---

## ğŸ›‘ Stop & Cleanup

```powershell
# Stop Minikube (keeps data)
minikube stop

# Delete everything
minikube delete

# Remove namespace
kubectl delete namespace air-quality
```

---

## ğŸ“Š Data Format

### Kafka Message
```json
{
  "timestamp": 1736912345,
  "longitude": 105.8542,
  "latitude": 21.0285,
  "temperature": 25.3,
  "feels_like": 27.1,
  "humidity": 75,
  "pressure": 1012,
  "city": "Hanoi"
}
```

### Cities & Coordinates
| City | Coordinates | Temp Range |
|------|-------------|-----------|
| HÃ  Ná»™i | 21.0285Â°N, 105.8542Â°E | 15-35Â°C |
| ÄÃ  Náºµng | 16.0544Â°N, 108.2022Â°E | 20-36Â°C |
| TP HCM | 10.8231Â°N, 106.6297Â°E | 25-38Â°C |

---

## ğŸ¯ Next Steps

1. âœ… Complete BÆ¯á»šC 1-10
2. ğŸ“Š Setup Grafana (Optional BÆ¯á»šC 11)
3. ğŸš€ Customize batch job for your needs
4. ğŸ“ˆ Add alerting rules
5. ğŸ”„ Setup automated CronJob for daily batch

---

**Last Updated:** 2026-01-16  
**Version:** 2.0  
**Status:** âœ… Production Ready
